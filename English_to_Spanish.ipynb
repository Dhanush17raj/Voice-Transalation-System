{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZfFtz0NTIjh"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R9EaPnoUm2L",
        "outputId": "ea2ce211-8a5f-4c1b-a343-aa9c4b1e51cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "metadata": {
        "id": "XqYve3ceUqJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file ='/content/spa.txt'"
      ],
      "metadata": {
        "id": "h_j_995tUxTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Data"
      ],
      "metadata": {
        "id": "otRWTJ2kV13H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
      ],
      "metadata": {
        "id": "_RKYah4dUxWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    #w = w.lower().strip()\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    #w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.lstrip().strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "metadata": {
        "id": "uRtlQmh8Uxal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    #lines = io.open('hin.txt', encoding='UTF-8').read().split('\\n')\n",
        "    #lines = lines.strip().split('\\n')\n",
        "    #lines = io.open(path, encoding='UTF-8').readlines().strip().split('\\n')\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "    return zip(*word_pairs)\n"
      ],
      "metadata": {
        "id": "UTLALauQWGRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
        "lines[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "BvfRGlsYWGUp",
        "outputId": "01f3eb46-dcd2-46b1-a4f8-b386da166371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"One day, I woke up to find that God had put hair on my face. I shaved it off. The next day, I found that God had put it back on my face, so I shaved it off again. On the third day, when I found that God had put hair back on my face again, I decided to let God have his way. That's why I have a beard.\\tUn día, me desperté y vi que Dios me había puesto pelo en la cara. Me lo afeité. Al día siguiente, vi que Dios me lo había vuelto a poner en la cara, así que me lo afeité otra vez. Al tercer día, cuando vi que Dios me había puesto pelo en la cara de nuevo, decidí que Dios se saliera con la suya. Por eso tengo barba.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #10104877 (CK) & #10106093 (manufrutos)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en, hn, cv = create_dataset(path_to_file, None)\n",
        "print(en[-2])\n",
        "print(hn[-2])\n",
        "print(cv[-2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj8rFlIqWGYa",
        "outputId": "97a2b3f6-cfe6-4f5e-a197-c4c9e2ff2e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> it may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort . however , if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning , we might be able to minimize errors . <end>\n",
            "<start> puede que sea imposible obtener un corpus completamente libre de errores debido a la naturaleza de este tipo de esfuerzo de colaboracion . sin embargo , si animamos a los miembros a contribuir frases en sus propios idiomas en lugar de experimentar con los idiomas que estan aprendiendo , podriamos ser capaces de minimizar los errores . <end>\n",
            "<start> cc-by 2 . 0 (france) attribution: tatoeba . org #2024159 (ck) & #4463195 (cueyayotl) <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "metadata": {
        "id": "4sRGCEgPXc7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "_tgeLB8kXdDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    inp_lang, targ_lang, _ = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "metadata": {
        "id": "Qr_-BAVyZDc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  To train faster, we can limit the size of the dataset to 3,000 sentences (of course, translation quality degrades with less data).\n",
        "#  Try experimenting with the size of the dataset\n",
        "num_examples = 70000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_inp, max_length_targ = max_length(input_tensor), max_length(target_tensor)"
      ],
      "metadata": {
        "id": "XAmnpgsOZDgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length_inp, max_length_targ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y39SXVCZDjg",
        "outputId": "d1b7effb-7507-4d23-9049-a1d9ece918cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n",
        "                                                                                                target_tensor,\n",
        "                                                                                                test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdHYK12eXdI7",
        "outputId": "47f126a8-7c27-464f-9712-6441fef743de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56000 56000 14000 14000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "metadata": {
        "id": "bJS7r_qiZe-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAHyHcXnZjfK",
        "outputId": "85222887-9fb8-4e69-ec82-837023de831e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,  11, 135, 263,   3,   2,   0,   0,   0,   0,   0,   0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('10:' + inp_lang.index_word[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CuJAF93Zjjd",
        "outputId": "1a75300b-c816-4508-ca67-c8188c505ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10:is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6XYauIVZjnt",
        "outputId": "a8eb737b-43b9-4775-f2a8-5aeb389b53da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "33 ----> be\n",
            "4306 ----> vigilant\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "1941 ----> estate\n",
            "6615 ----> vigilante\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(inp_lang.word_index)+1, len(targ_lang.word_index)+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQkcer0SZy6i",
        "outputId": "10816fcc-da0e-4295-f45f-49187814bba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8433, 16283)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "miK1kYpcZy9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZJnS3Z3ZfBm",
        "outputId": "db68b81d-12fc-40e4-d697-00d69a3c9f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,   33, 4306,    3,    2,    0,    0,    0,    0,    0,    0,\n",
              "          0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1svEkgCqaOOG",
        "outputId": "1d5bb761-f398-41f5-cd3f-9cbdaed978c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 12]), TensorShape([32, 21]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch[0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3w1czRHaOQw",
        "outputId": "eb3733e1-38cd-4ebe-9bb8-4924c7adc5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
              "array([[   1,    7, 1412,    8, 3252,    3,    2,    0,    0,    0,    0,\n",
              "           0],\n",
              "       [   1,   44,  422,   23,   54,   47,  145,    3,    2,    0,    0,\n",
              "           0]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True,\n",
        "                                        return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "metadata": {
        "id": "ZDSBCUBvaOTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEjLLyaEaOXH",
        "outputId": "d19a7b42-9554-49c9-cb6f-4f2a0807fdcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (32, 12, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (32, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "rYjtIck6bJ_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6Imn8BfbKCz",
        "outputId": "21754477-2c61-4e4d-8914-3dce0d2f0c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch size, units) (32, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (32, 12, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True,\n",
        "                                        return_state=True, recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "metadata": {
        "id": "QWwCCNVwbKFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((32, 1)), sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d15ryqtFbKIw",
        "outputId": "e5993726-ff0a-4b10-fbae-ec79ca13d79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (32, 16283)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "NmoaLJvIbg8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "P6-rdO1CbcrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "metadata": {
        "id": "OHPWa0zgbcu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "67HxiOQubKLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        #if batch % 100 == 0:\n",
        "            #print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcwt0IdhZfEG",
        "outputId": "108882f2-4963-4304-ba35-f071f888b54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 1.1965\n",
            "Time taken for 1 epoch 244.43194723129272 sec\n",
            "\n",
            "Epoch 2 Loss 0.6781\n",
            "Time taken for 1 epoch 212.61538195610046 sec\n",
            "\n",
            "Epoch 3 Loss 0.4221\n",
            "Time taken for 1 epoch 211.3305060863495 sec\n",
            "\n",
            "Epoch 4 Loss 0.2875\n",
            "Time taken for 1 epoch 217.30458641052246 sec\n",
            "\n",
            "Epoch 5 Loss 0.2109\n",
            "Time taken for 1 epoch 211.4978895187378 sec\n",
            "\n",
            "Epoch 6 Loss 0.1664\n",
            "Time taken for 1 epoch 217.69576859474182 sec\n",
            "\n",
            "Epoch 7 Loss 0.1384\n",
            "Time taken for 1 epoch 211.42755889892578 sec\n",
            "\n",
            "Epoch 8 Loss 0.1205\n",
            "Time taken for 1 epoch 217.72517085075378 sec\n",
            "\n",
            "Epoch 9 Loss 0.1075\n",
            "Time taken for 1 epoch 211.438138961792 sec\n",
            "\n",
            "Epoch 10 Loss 0.0981\n",
            "Time taken for 1 epoch 217.84624600410461 sec\n",
            "\n",
            "Epoch 11 Loss 0.0915\n",
            "Time taken for 1 epoch 211.4702718257904 sec\n",
            "\n",
            "Epoch 12 Loss 0.0853\n",
            "Time taken for 1 epoch 217.60408401489258 sec\n",
            "\n",
            "Epoch 13 Loss 0.0817\n",
            "Time taken for 1 epoch 211.37652850151062 sec\n",
            "\n",
            "Epoch 14 Loss 0.0783\n",
            "Time taken for 1 epoch 217.5667209625244 sec\n",
            "\n",
            "Epoch 15 Loss 0.0764\n",
            "Time taken for 1 epoch 211.39322113990784 sec\n",
            "\n",
            "Epoch 16 Loss 0.0736\n",
            "Time taken for 1 epoch 214.7797839641571 sec\n",
            "\n",
            "Epoch 17 Loss 0.0714\n",
            "Time taken for 1 epoch 211.16026067733765 sec\n",
            "\n",
            "Epoch 18 Loss 0.0696\n",
            "Time taken for 1 epoch 217.53679871559143 sec\n",
            "\n",
            "Epoch 19 Loss 0.0682\n",
            "Time taken for 1 epoch 211.22436928749084 sec\n",
            "\n",
            "Epoch 20 Loss 0.0670\n",
            "Time taken for 1 epoch 217.46832823753357 sec\n",
            "\n",
            "Epoch 21 Loss 0.0657\n",
            "Time taken for 1 epoch 211.04340052604675 sec\n",
            "\n",
            "Epoch 22 Loss 0.0647\n",
            "Time taken for 1 epoch 217.41678953170776 sec\n",
            "\n",
            "Epoch 23 Loss 0.0632\n",
            "Time taken for 1 epoch 211.21626210212708 sec\n",
            "\n",
            "Epoch 24 Loss 0.0621\n",
            "Time taken for 1 epoch 217.1037256717682 sec\n",
            "\n",
            "Epoch 25 Loss 0.0610\n",
            "Time taken for 1 epoch 211.14654541015625 sec\n",
            "\n",
            "Epoch 26 Loss 0.0606\n",
            "Time taken for 1 epoch 217.39468812942505 sec\n",
            "\n",
            "Epoch 27 Loss 0.0598\n",
            "Time taken for 1 epoch 211.16460061073303 sec\n",
            "\n",
            "Epoch 28 Loss 0.0589\n",
            "Time taken for 1 epoch 217.3636302947998 sec\n",
            "\n",
            "Epoch 29 Loss 0.0579\n",
            "Time taken for 1 epoch 211.0342583656311 sec\n",
            "\n",
            "Epoch 30 Loss 0.0577\n",
            "Time taken for 1 epoch 216.95415496826172 sec\n",
            "\n",
            "Epoch 31 Loss 0.0567\n",
            "Time taken for 1 epoch 211.37887167930603 sec\n",
            "\n",
            "Epoch 32 Loss 0.0564\n",
            "Time taken for 1 epoch 222.01797103881836 sec\n",
            "\n",
            "Epoch 33 Loss 0.0553\n",
            "Time taken for 1 epoch 211.3535475730896 sec\n",
            "\n",
            "Epoch 34 Loss 0.0548\n",
            "Time taken for 1 epoch 221.95215606689453 sec\n",
            "\n",
            "Epoch 35 Loss 0.0549\n",
            "Time taken for 1 epoch 211.1392662525177 sec\n",
            "\n",
            "Epoch 36 Loss 0.0535\n",
            "Time taken for 1 epoch 216.86815190315247 sec\n",
            "\n",
            "Epoch 37 Loss 0.0530\n",
            "Time taken for 1 epoch 211.19449734687805 sec\n",
            "\n",
            "Epoch 38 Loss 0.0524\n",
            "Time taken for 1 epoch 212.88040280342102 sec\n",
            "\n",
            "Epoch 39 Loss 0.0521\n",
            "Time taken for 1 epoch 211.08528447151184 sec\n",
            "\n",
            "Epoch 40 Loss 0.0517\n",
            "Time taken for 1 epoch 217.30314564704895 sec\n",
            "\n",
            "Epoch 41 Loss 0.0509\n",
            "Time taken for 1 epoch 211.08185744285583 sec\n",
            "\n",
            "Epoch 42 Loss 0.0511\n",
            "Time taken for 1 epoch 220.36244797706604 sec\n",
            "\n",
            "Epoch 43 Loss 0.0497\n",
            "Time taken for 1 epoch 210.9797637462616 sec\n",
            "\n",
            "Epoch 44 Loss 0.0499\n",
            "Time taken for 1 epoch 217.43066096305847 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "metadata": {
        "id": "7ljf4Kx3cBa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    cax = ax.matshow(attention, cmap='viridis')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2ipfxcVXcBdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ],
      "metadata": {
        "id": "O6-56OyrcWS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dire ="
      ],
      "metadata": {
        "id": "1cQCBUDiqFHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqzD5xvwcXEb",
        "outputId": "a8806c5e-ccf6-4ad1-f046-2026a4050f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x788eb956e590>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_words_not_in_dict(sentence, inp_lang):\n",
        "    words = sentence.split()\n",
        "    lower_case_word_index = {word.lower() for word in inp_lang.word_index}\n",
        "    filtered_words = [word for word in words if word.lower() in lower_case_word_index]\n",
        "    filtered_sentence = ' '.join(filtered_words)\n",
        "    return filtered_sentence\n",
        "filtered_sentence = filter_words_not_in_dict(\"this is a test sentence cav not present\", inp_lang)\n"
      ],
      "metadata": {
        "id": "aI0KWTGN4j0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RS8qceTca36",
        "outputId": "402f526f-c286-4e85-d1ed-233f414a60d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> this is a test sentence not present <end>\n",
            "Predicted translation: esta es una prueba esta creas conveniente . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"This is a test sentence not present\")"
      ],
      "metadata": {
        "id": "BZtj4nc9ca7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00884cd-d1a2-4db8-92fa-755eaa242966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> this is a test sentence not present <end>\n",
            "Predicted translation: esta es una prueba esta creas conveniente . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PGaeoeSBAfCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlH7XhvqAfJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uwOnhbQXAfL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import load_model\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import io\n",
        "import time\n",
        "from flask import Flask, request, render_template,url_for, redirect\n",
        "\n",
        "app = Flask(__name__,static_url_path='/static')\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template(\"audio-only.html\")\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == \"POST\":\n",
        "        if 'file' not in request.files:\n",
        "            return \"No file part\"\n",
        "        audioFile = request.files['file']\n",
        "\n",
        "        directory_path = \"/home/dhanush/Documents/ML/MinorProject\"\n",
        "        if not os.path.exists(directory_path):\n",
        "            os.makedirs(directory_path)\n",
        "\n",
        "        file_path = os.path.join(directory_path, audioFile.filename)\n",
        "        audioFile.save(file_path)\n",
        "        frame_length = 256\n",
        "        frame_step = 160\n",
        "        fft_length = 384\n",
        "        characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
        "\n",
        "        # Mapping characters to integers\n",
        "        char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "\n",
        "        # Mapping integers back to original characters\n",
        "        num_to_char = keras.layers.StringLookup(\n",
        "        vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        "        )\n",
        "        batch_size = 32\n",
        "        def decode_batch_predictions(pred):\n",
        "            input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "        # Use greedy search. For complex tasks, you can use beam search\n",
        "            results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "        # Iterate over the results and get back the text\n",
        "            output_text = []\n",
        "            for result in results:\n",
        "                result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "                output_text.append(result)\n",
        "            return output_text\n",
        "        def encode_single_sample(wav_file):\n",
        "        # Process the Audio\n",
        "        # 1. Read wav file\n",
        "            file = tf.io.read_file(wav_file)\n",
        "        # 2. Decode the wav file\n",
        "            audio, _ = tf.audio.decode_wav(file)\n",
        "        # 3. Squeeze the tensor along the channel axis\n",
        "            audio = tf.squeeze(audio, axis=-1)\n",
        "        # 4. Change type to float\n",
        "            # if len(audio.shape) > 1:\n",
        "            # # If there's an extra dimension, select the first channel\n",
        "            #     audio = audio[:, 0]\n",
        "            audio = tf.cast(audio, tf.float32)\n",
        "        # 5. Get the spectrogram\n",
        "            spectrogram = tf.signal.stft(\n",
        "                audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "            )\n",
        "        # 6. We only need the magnitude, which can be derived by applying tf.abs\n",
        "            spectrogram = tf.abs(spectrogram)\n",
        "            spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "        # 7. Normalization\n",
        "            means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "            stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "            spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "            return spectrogram\n",
        "        def CTCLoss(y_true, y_pred):\n",
        "        # Compute the training-time loss value\n",
        "            batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "            input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "            label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "            input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "            label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "            loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "            return loss\n",
        "        def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
        "            \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
        "            # Model's input\n",
        "            input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "            # Expand the dimension to use 2D CNN.\n",
        "            x = layers.Reshape((-1, input_dim, 1), input_shape=(None, input_dim), name=\"expand_dim\")(input_spectrogram)\n",
        "            # Convolution layer 1\n",
        "            x = layers.Conv2D(\n",
        "                filters=32,\n",
        "                kernel_size=[11, 41],\n",
        "                strides=[2, 2],\n",
        "                padding=\"same\",\n",
        "                use_bias=False,\n",
        "                name=\"conv_1\",\n",
        "            )(x)\n",
        "            x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
        "            x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
        "            # Convolution layer 2\n",
        "            x = layers.Conv2D(\n",
        "                filters=32,\n",
        "                kernel_size=[11, 21],\n",
        "                strides=[1, 2],\n",
        "                padding=\"same\",\n",
        "                use_bias=False,\n",
        "                name=\"conv_2\",\n",
        "            )(x)\n",
        "            x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
        "            x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
        "            # Reshape the resulted volume to feed the RNNs layers\n",
        "            x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "            # RNN layers\n",
        "            for i in range(1, rnn_layers + 1):\n",
        "                recurrent = layers.GRU(\n",
        "                    units=rnn_units,\n",
        "                    activation=\"tanh\",\n",
        "                    recurrent_activation=\"sigmoid\",\n",
        "                    use_bias=True,\n",
        "                    return_sequences=True,\n",
        "                    reset_after=True,\n",
        "                    name=f\"gru_{i}\",\n",
        "                )\n",
        "                x = layers.Bidirectional(\n",
        "                    recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
        "                )(x)\n",
        "                if i < rnn_layers:\n",
        "                    x = layers.Dropout(rate=0.5)(x)\n",
        "            # Dense layer\n",
        "            x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
        "            x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "            x = layers.Dropout(rate=0.5)(x)\n",
        "            # Classification layer\n",
        "            output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "            # Model\n",
        "            model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "            # Optimizer\n",
        "            opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "            # Compile the model and return\n",
        "            model.compile(optimizer=opt, loss=CTCLoss)\n",
        "            return model\n",
        "\n",
        "        model1 = load_model('speech_recognition_model.h5', custom_objects={'CTCLoss': CTCLoss})\n",
        "\n",
        "        wav_file=audioFile\n",
        "\n",
        "        def decode_batch_predictions(pred, beam_width=5):\n",
        "            input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "            # Use beam search instead of greedy search\n",
        "            results = keras.backend.ctc_decode(pred, input_length=input_len, beam_width=beam_width, top_paths=1)[0][0]\n",
        "            # Iterate over the results and get back the text\n",
        "            output_text = []\n",
        "            for result in results:\n",
        "                result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "                output_text.append(result)\n",
        "            return output_text\n",
        "        X =encode_single_sample(file_path)\n",
        "        X = tf.expand_dims(X, axis=0)\n",
        "        batch_predictions = model1.predict(X)\n",
        "        batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "        # print(batch_predictions)\n",
        "        path_to_file ='spa.txt'\n",
        "        def unicode_to_ascii(s):\n",
        "            return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "        def preprocess_sentence(w):\n",
        "            w = unicode_to_ascii(w.lower().strip())\n",
        "        #w = w.lower().strip()\n",
        "\n",
        "        # creating a space between a word and the punctuation following it\n",
        "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "            w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "            w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "            # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "            #w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "            w = w.lstrip().strip()\n",
        "\n",
        "            # adding a start and an end token to the sentence\n",
        "            # so that the model know when to start and stop predicting.\n",
        "            w = '<start> ' + w + ' <end>'\n",
        "            return w\n",
        "        def create_dataset(path, num_examples):\n",
        "        #lines = io.open('hin.txt', encoding='UTF-8').read().split('\\n')\n",
        "        #lines = lines.strip().split('\\n')\n",
        "        #lines = io.open(path, encoding='UTF-8').readlines().strip().split('\\n')\n",
        "            lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "            word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "            return zip(*word_pairs)\n",
        "        def max_length(tensor):\n",
        "            return max(len(t) for t in tensor)\n",
        "        def tokenize(lang):\n",
        "            lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "            lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "            tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "            tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "            return tensor, lang_tokenizer\n",
        "        def load_dataset(path, num_examples=None):\n",
        "        # creating cleaned input, output pairs\n",
        "            inp_lang, targ_lang, _ = create_dataset(path, num_examples)\n",
        "\n",
        "            input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "            target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "            return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "        num_examples = 3000\n",
        "        input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "        # Calculate max_length of the target tensors\n",
        "        max_length_inp, max_length_targ = max_length(input_tensor), max_length(target_tensor)\n",
        "\n",
        "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n",
        "                                                                                                    target_tensor,\n",
        "                                                                                                    test_size=0.2)\n",
        "        def convert(lang, tensor):\n",
        "            for t in tensor:\n",
        "                if t!=0:\n",
        "                    print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "        BUFFER_SIZE = len(input_tensor_train)\n",
        "        BATCH_SIZE = 32\n",
        "        steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "        embedding_dim = 256\n",
        "        units = 1024\n",
        "        vocab_inp_size = len(inp_lang.word_index)+1\n",
        "        vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "        dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "        example_input_batch, example_target_batch = next(iter(dataset))\n",
        "        class Encoder(tf.keras.Model):\n",
        "            def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "                super(Encoder, self).__init__()\n",
        "                self.batch_sz = batch_sz\n",
        "                self.enc_units = enc_units\n",
        "                self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "                self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True,\n",
        "                                                return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "            def call(self, x, hidden):\n",
        "                x = self.embedding(x)\n",
        "                output, state = self.gru(x, initial_state = hidden)\n",
        "                return output, state\n",
        "\n",
        "            def initialize_hidden_state(self):\n",
        "                return tf.zeros((self.batch_sz, self.enc_units))\n",
        "        encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "        class BahdanauAttention(tf.keras.Model):\n",
        "            def __init__(self, units):\n",
        "                super(BahdanauAttention, self).__init__()\n",
        "                self.W1 = tf.keras.layers.Dense(units)\n",
        "                self.W2 = tf.keras.layers.Dense(units)\n",
        "                self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "            def call(self, query, values):\n",
        "                # hidden shape == (batch_size, hidden size)\n",
        "                # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "                # we are doing this to perform addition to calculate the score\n",
        "                hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "                # score shape == (batch_size, max_length, 1)\n",
        "                # we get 1 at the last axis because we are applying score to self.V\n",
        "                # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "                score = self.V(tf.nn.tanh(\n",
        "                    self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "                # attention_weights shape == (batch_size, max_length, 1)\n",
        "                attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "                # context_vector shape after sum == (batch_size, hidden_size)\n",
        "                context_vector = attention_weights * values\n",
        "                context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "                return context_vector, attention_weights\n",
        "        class Decoder(tf.keras.Model):\n",
        "            def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "                super(Decoder, self).__init__()\n",
        "                self.batch_sz = batch_sz\n",
        "                self.dec_units = dec_units\n",
        "                self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "                self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True,\n",
        "                                                return_state=True, recurrent_initializer='glorot_uniform')\n",
        "                self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "                # used for attention\n",
        "                self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "            def call(self, x, hidden, enc_output):\n",
        "                # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "                context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "                # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "                x = self.embedding(x)\n",
        "\n",
        "                # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "                x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "                # passing the concatenated vector to the GRU\n",
        "                output, state = self.gru(x)\n",
        "\n",
        "                # output shape == (batch_size * 1, hidden_size)\n",
        "                output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "                # output shape == (batch_size, vocab)\n",
        "                x = self.fc(output)\n",
        "                return x, state, attention_weights\n",
        "\n",
        "        decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam()\n",
        "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "        def loss_function(real, pred):\n",
        "            mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "            loss_ = loss_object(real, pred)\n",
        "\n",
        "            mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "            loss_ *= mask\n",
        "\n",
        "            return tf.reduce_mean(loss_)\n",
        "        checkpoint_dir = './training_checkpoints'\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "        checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "        @tf.function\n",
        "        def train_step(inp, targ, enc_hidden):\n",
        "            loss  = 0\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "                dec_hidden = enc_hidden\n",
        "\n",
        "                dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "                # Teacher forcing - feeding the target as the next input\n",
        "                for t in range(1, targ.shape[1]):\n",
        "                    # passing enc_output to the decoder\n",
        "                    predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "                    loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "                    # using teacher forcing\n",
        "                    dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "            batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "            variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "            gradients = tape.gradient(loss, variables)\n",
        "\n",
        "            optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "            return batch_loss\n",
        "        def evaluate(sentence):\n",
        "            attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "            sentence = preprocess_sentence(sentence)\n",
        "\n",
        "            inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "            inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "            inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "            result = ''\n",
        "\n",
        "            hidden = [tf.zeros((1, units))]\n",
        "            enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "            dec_hidden = enc_hidden\n",
        "            dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "            for t in range(max_length_targ):\n",
        "                predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "                # storing the attention weights to plot later on\n",
        "                attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "                attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "                predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "                result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "                if targ_lang.index_word[predicted_id] == '<end>':\n",
        "                    return result, sentence, attention_plot\n",
        "\n",
        "                # the predicted ID is fed back into the model\n",
        "                dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "            return result, sentence, attention_plot\n",
        "        def translate(sentence):\n",
        "            result, sentence, attention_plot = evaluate(sentence)\n",
        "            return result\n",
        "        checkpoint_dir='training_checkpoints'\n",
        "\n",
        "        checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "        def filter_words_not_in_dict(sentence, inp_lang):\n",
        "            words = sentence.split()\n",
        "            lower_case_word_index = {word.lower() for word in inp_lang.word_index}\n",
        "            filtered_words = [word for word in words if word.lower() in lower_case_word_index]\n",
        "            filtered_sentence = ' '.join(filtered_words)\n",
        "            return filtered_sentence\n",
        "        filtered_sentence = filter_words_not_in_dict(batch_predictions[0], inp_lang)\n",
        "        output=translate(filtered_sentence)\n",
        "\n",
        "\n",
        "        def text_to_sound(text, language='hi', filename='output.mp3'):\n",
        "            # Create a gTTS object\n",
        "            tts = gTTS(text=text, lang=language, slow=False)\n",
        "\n",
        "            # Save the speech as an MP3 file\n",
        "            tts.save(filename)\n",
        "        text_to_sound(output)\n",
        "    zrr= \"Output is here:\"\n",
        "    return render_template(\"audio-only.html\", pred=zrr)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "flJitrV_AXBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"utf-8\">\n",
        "    <title>Audio-only Example - Record Plugin for Video.js</title>\n",
        "\n",
        "    <!-- Existing CSS links -->\n",
        "    <!-- <link href=\"../node_modules/video.js/dist/video-js.min.css\" rel=\"stylesheet\">\n",
        "    <link href=\"../node_modules/videojs-wavesurfer/dist/css/videojs.wavesurfer.min.css\" rel=\"stylesheet\">\n",
        "    <link href=\"../dist/css/videojs.record.css\" rel=\"stylesheet\">\n",
        "    <link href=\"assets/css/examples.css\" rel=\"stylesheet\"> -->\n",
        "\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/video-js.min.css') }}\">\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/videojs.wavesurfer.min.css') }}\">\n",
        "\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/videojs.record.css') }}\">\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/examples.css') }}\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    <!-- Additional CSS for the players and heading -->\n",
        "    <style>\n",
        "        body {\n",
        "            margin: 0;\n",
        "            padding: 0;\n",
        "            background-image: url(\"static/Images/pexels-pixabay-355747.jpg\");\n",
        "            background-size: cover;\n",
        "            background-position: center;\n",
        "            font-family: Arial, Helvetica, sans-serif;\n",
        "            height: 100vh;\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "            align-items: center;\n",
        "            justify-content: center;\n",
        "        }\n",
        "\n",
        "        #heading {\n",
        "            color: black;\n",
        "            font-size: 4em; /* Increased size by 2 times */\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "\n",
        "        #myAudio,\n",
        "        #voicePlayer,\n",
        "        #audioPlayer {\n",
        "            margin-top: 25px;\n",
        "            margin-bottom: 65px;\n",
        "            width: 50%;\n",
        "            z-index: 2;\n",
        "            background-color: rgba(255, 131, 0, 0);\n",
        "        }\n",
        "         /* Add the following styles to darken the play button and player bar */\n",
        "         .vjs-button-control.vjs-play-control:before {\n",
        "            color: #333; /* Darkened color (replace with your preferred color) */\n",
        "        }\n",
        "\n",
        "        #myAudio .vjs-play-progress,\n",
        "        #myAudio .vjs-volume-level {\n",
        "            background-color: #050; /* Darkened color (replace with your preferred color) */\n",
        "        }\n",
        "\n",
        "        #myAudio .vjs-control-bar {\n",
        "            background-color: #333; /* Darkened color (replace with your preferred color) */\n",
        "        }\n",
        "\n",
        "        #button-container {\n",
        "    display: flex;\n",
        "}\n",
        "\n",
        "#chooseFileLabel {\n",
        "    background-color: black;\n",
        "    color: white;\n",
        "    font-size: 1.3em;\n",
        "    padding: 7px 8px;\n",
        "    border: none;\n",
        "    cursor: pointer;\n",
        "    border-radius: 35px;\n",
        "    margin-left: 0; /* Adjusted to shift the button to the left */\n",
        "\n",
        "}\n",
        "\n",
        "#fileDisplay {\n",
        "    color: black;\n",
        "    font-size: 1.5em;\n",
        "    font-weight: bold; /* Added to make text bolder */\n",
        "    padding: 10px;\n",
        "    margin-left: 10px;\n",
        "}\n",
        "\n",
        "#upload {\n",
        "    background-color: black;\n",
        "    color: white;\n",
        "    font-size: 1.3em;\n",
        "    padding: 7px 8px;\n",
        "    border: none;\n",
        "    cursor: pointer;\n",
        "    border-radius: 35px;\n",
        "}\n",
        "\n",
        "input[type=\"file\"] {\n",
        "    display: none;\n",
        "}\n",
        "    </style>\n",
        "\n",
        "    <!-- Existing JavaScript links -->\n",
        "    <!-- <script src=\"../node_modules/video.js/dist/video.min.js\"></script>\n",
        "    <script src=\"../node_modules/recordrtc/RecordRTC.js\"></script>\n",
        "    <script src=\"../node_modules/webrtc-adapter/out/adapter.js\"></script>\n",
        "    <script src=\"../node_modules/wavesurfer.js/dist/wavesurfer.min.js\"></script>\n",
        "    <script src=\"../node_modules/wavesurfer.js/dist/plugin/wavesurfer.microphone.min.js\"></script>\n",
        "    <script src=\"../node_modules/videojs-wavesurfer/dist/videojs.wavesurfer.min.js\"></script>\n",
        "\n",
        "    <script src=\"../dist/videojs.record.js\"></script>\n",
        "\n",
        "    <script src=\"browser-workarounds.js\"></script> -->\n",
        "    <script src=\"{{ url_for('static', filename='video.js/dist/video.min.js') }}\"></script>\n",
        "    <script src=\"{{ url_for('static', filename='recordrtc/RecordRTC.js') }}\"></script>\n",
        "    <script src=\"{{ url_for('static', filename='webrtc-adapter/out/adapter.js') }}\"></script>\n",
        "    <script src=\"{{ url_for('static', filename='wavesurfer.js/dist/wavesurfer.min.js') }}\"></script>\n",
        "\n",
        "    <script src=\"{{ url_for('static', filename='wavesurfer.js/dist/plugin/wavesurfer.microphone.min.js') }}\"></script>\n",
        "    <script src=\"{{ url_for('static', filename='videojs-wavesurfer/dist/videojs.wavesurfer.min.js') }}\"></script>\n",
        "    <script src=\"{{ url_for('static', filename='videojs.record.js') }}\"></script>\n",
        "\n",
        "\n",
        "    <script src=\"{{ url_for('static', filename='browser-workarounds.js') }}\"></script>\n",
        "\n",
        "\n",
        "    <!-- Existing Style -->\n",
        "    <style>\n",
        "        /* Change player background color */\n",
        "        #myAudio {\n",
        "            background-color: (0, 0, 0, 0.5);\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<!-- Aesthetic Heading -->\n",
        "<h1 id=\"heading\">Voice Translation System</h1>\n",
        "\n",
        "<audio id=\"myAudio\" class=\"video-js vjs-default-skin\"></audio>\n",
        "<!-- <audio id=\"audioPlayer\" controls class=\"video-js vjs-default-skin\"></audio> -->\n",
        "\n",
        "<audio id=\"voicePlayer\" class=\"video-js vjs-default-skin\"></audio>\n",
        "\n",
        "<div id=\"button-container\">\n",
        "    <form method=\"POST\" action=\"{{url_for('predict')}}\" enctype=\"multipart/form-data\">\n",
        "        <label for=\"fileInput\" id=\"chooseFileLabel\">Choose file</label>\n",
        "        <div class=\"file-display\" id=\"fileDisplay\"></div>\n",
        "        <input id=\"fileInput\" name=\"file\" type=\"file\">\n",
        "        <button id=\"upload\" type=\"submit\" value = \"predict\">Upload</button>\n",
        "        <h4 style=\"color: antiquewhite; position:absolute; top: 30%; left: 50%; font-size: 1cm\">{{ pred }}</h4>\n",
        "    </form>\n",
        "</div>\n",
        "<audio id=\"audioPlayer\" controls class=\"video-js vjs-default-skin\"></audio>\n",
        "\n",
        "<script>\n",
        "    /* eslint-disable */\n",
        "    var options = {\n",
        "        controls: true,\n",
        "        bigPlayButton: false,\n",
        "        width: 600,\n",
        "        height: 300,\n",
        "        fluid: false,\n",
        "        plugins: {\n",
        "            wavesurfer: {\n",
        "                backend: 'WebAudio',\n",
        "                waveColor: '#36393b',\n",
        "                progressColor: 'black',\n",
        "                displayMilliseconds: true,\n",
        "                debug: true,\n",
        "                cursorWidth: 1,\n",
        "                hideScrollbar: true,\n",
        "                plugins: [\n",
        "                    // enable microphone plugin\n",
        "                    WaveSurfer.microphone.create({\n",
        "                        bufferSize: 4096,\n",
        "                        numberOfInputChannels: 1,\n",
        "                        numberOfOutputChannels: 1,\n",
        "                        constraints: {\n",
        "                            video: false,\n",
        "                            audio: true\n",
        "                        }\n",
        "                    })\n",
        "                ]\n",
        "            },\n",
        "            record: {\n",
        "                audio: true,\n",
        "                video: false,\n",
        "                maxLength: 6,\n",
        "                displayMilliseconds: true,\n",
        "                debug: true\n",
        "            }\n",
        "        }\n",
        "    };\n",
        "\n",
        "    // apply audio workarounds for certain browsers\n",
        "    applyAudioWorkaround();\n",
        "\n",
        "    // create player\n",
        "    var player = videojs('myAudio', options, function() {\n",
        "        // print version information at startup\n",
        "        var msg = 'Using video.js ' + videojs.VERSION +\n",
        "            ' with videojs-record ' + videojs.getPluginVersion('record') +\n",
        "            ', videojs-wavesurfer ' + videojs.getPluginVersion('wavesurfer') +\n",
        "            ', wavesurfer.js ' + WaveSurfer.VERSION + ' and recordrtc ' +\n",
        "            RecordRTC.version;\n",
        "        videojs.log(msg);\n",
        "    });\n",
        "    var fileInput = document.getElementById('fileInput');\n",
        "    var fileDisplay = document.getElementById('fileDisplay');\n",
        "\n",
        "    fileInput.addEventListener('change', function () {\n",
        "        // Check if files are selected\n",
        "        if (fileInput.files.length > 0) {\n",
        "            // Display the first selected file name\n",
        "            fileDisplay.textContent = fileInput.files[0].name;\n",
        "        } else {\n",
        "            // If no file is selected, clear the display\n",
        "            fileDisplay.textContent = '';\n",
        "        }\n",
        "    });\n",
        "\n",
        "    // error handling\n",
        "    player.on('deviceError', function() {\n",
        "        console.log('device error:', player.deviceErrorCode);\n",
        "    });\n",
        "\n",
        "    player.on('error', function(element, error) {\n",
        "        console.error(error);\n",
        "    });\n",
        "\n",
        "    // user clicked the record button and started recording\n",
        "    player.on('startRecord', function() {\n",
        "        console.log('started recording!');\n",
        "    });\n",
        "\n",
        "    // user completed recording and stream is available\n",
        "    player.on('finishRecord', function() {\n",
        "        // the blob object contains the recorded data that\n",
        "        // can be downloaded by the user, stored on the server, etc.\n",
        "        console.log('finished recording: ', player.recordedData);\n",
        "    });\n",
        "    player.on('finishRecord', function () {\n",
        "        // Check if recorded data is available and is a Blob\n",
        "        if (player.recordedData instanceof Blob) {\n",
        "            var blob = player.recordedData;\n",
        "            var url = URL.createObjectURL(blob);\n",
        "\n",
        "            // Create a link element and trigger a download\n",
        "            var a = document.createElement('a');\n",
        "            a.href = url;\n",
        "            a.download = 'recorded-audio.wav'; // You can change the filename and extension\n",
        "            a.click();\n",
        "\n",
        "            // Release the object URL after the download\n",
        "            URL.revokeObjectURL(url);\n",
        "        } else {\n",
        "            console.log('No valid recorded data available.');\n",
        "        }\n",
        "    });\n",
        "    // Create a new instance of AudioContext for the voice player\n",
        "    var audioContext = new (window.AudioContext || window.webkitAudioContext)();\n",
        "\n",
        "    // Create an audio element for the voice player\n",
        "    var voicePlayer = document.getElementById('voicePlayer');\n",
        "\n",
        "    // Load a sample WAV file for the voice player\n",
        "    fetch(\"/home/dhanush/Documents/ML/MinorProject/output.mp3\") // Replace with the actual path\n",
        "        .then(response => response.arrayBuffer())\n",
        "        .then(buffer => audioContext.decodeAudioData(buffer))\n",
        "        .then(decodedData => {\n",
        "            var source = audioContext.createBufferSource();\n",
        "            source.buffer = decodedData;\n",
        "            source.connect(audioContext.destination);\n",
        "            voicePlayer.srcObject = audioContext.createMediaStreamDestination().stream;\n",
        "            source.connect(audioContext.createMediaStreamDestination());\n",
        "            source.start();\n",
        "        })\n",
        "        .catch(error => console.error('Error loading WAV file:', error));\n",
        "\n",
        "    // Set the source of the audio player to a sample MP3 file\n",
        "    document.getElementById('audioPlayer').src = \"/home/dhanush/Documents/ML/MinorProject/output.mp3\";\n",
        "</script>\n",
        "</body>\n",
        "</html>"
      ],
      "metadata": {
        "id": "aPuHgG_XAYRu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}